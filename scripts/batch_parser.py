#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import json
import hashlib
from pathlib import Path
from datetime import datetime
import argparse

# ê¸°ì¡´ íŒŒì„œ ëª¨ë“ˆ import
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from parsers.advanced_parser import parse_pdf, parse_docx, parse_pptx, parse_xlsx

def get_file_hash(file_path):
    """íŒŒì¼ í•´ì‹œ ê³„ì‚° (ë³€ê²½ ê°ì§€ìš©)"""
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def load_processing_log():
    """ì²˜ë¦¬ ë¡œê·¸ ë¡œë“œ"""
    log_path = Path('data/processed/processing_log.json')
    print(f"ë¡œê·¸ íŒŒì¼ ê²½ë¡œ: {log_path}")

    if log_path.exists():
        print("ê¸°ì¡´ ë¡œê·¸ íŒŒì¼ ë°œê²¬, ë¡œë”© ì¤‘...")
        with open(log_path, 'r', encoding='utf-8') as f:
            log_data = json.load(f)
        print(f"ë¡œê·¸ íŒŒì¼ì—ì„œ {len(log_data)}ê°œ í•­ëª© ë¡œë“œë¨")
        return log_data
    else:
        print("ë¡œê·¸ íŒŒì¼ì´ ì—†ìŒ, ìƒˆë¡œ ì‹œì‘")
        return {}

def save_processing_log(log_data):
    """ì²˜ë¦¬ ë¡œê·¸ ì €ì¥"""
    log_path = Path('data/processed/processing_log.json')
    print(f"ë¡œê·¸ ì €ì¥ ì¤‘: {log_path}")
    log_path.parent.mkdir(parents=True, exist_ok=True)

    with open(log_path, 'w', encoding='utf-8') as f:
        json.dump(log_data, f, ensure_ascii=False, indent=2)
    print("ë¡œê·¸ ì €ì¥ ì™„ë£Œ")

def should_process_file(file_path, processing_log, force_all=False):
    """íŒŒì¼ ì²˜ë¦¬ í•„ìš” ì—¬ë¶€ í™•ì¸"""
    file_str = str(file_path)
    file_name = file_path.name

    print(f"íŒŒì¼ ê²€ì‚¬: {file_name}")

    # 1. ë©”ëª¨ íŒŒì¼ ì œì™¸ (ìš°ì„  í™•ì¸)
    if file_name.startswith('_') and file_path.suffix.lower() in ['.md', '.mdx']:
        print(f"  â†’ ë©”ëª¨ íŒŒì¼ ì œì™¸: {file_name}")
        return False

    # 2. ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹ë§Œ
    supported_extensions = ['.pdf', '.docx', '.pptx', '.xlsx', '.md', '.mdx', '.js', '.ts']
    if file_path.suffix.lower() not in supported_extensions:
        print(f"  â†’ ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {file_path.suffix}")
        return False

    # 3. ì‹œìŠ¤í…œ íŒŒì¼ ì œì™¸
    system_files = ['.DS_Store', 'Thumbs.db', '.gitkeep']
    if file_name in system_files:
        print(f"  â†’ ì‹œìŠ¤í…œ íŒŒì¼ ì œì™¸: {file_name}")
        return False

    # 4. ê°•ì œ ì¬ì²˜ë¦¬ ëª¨ë“œ
    if force_all:
        print(f"  â†’ ê°•ì œ ì²˜ë¦¬ ëª¨ë“œ: {file_name}")
        return True

    # 5. í•´ì‹œ ë¹„êµë¡œ ë³€ê²½ ê°ì§€
    try:
        current_hash = get_file_hash(file_path)
        if file_str not in processing_log:
            print(f"  â†’ ìƒˆ íŒŒì¼: {file_name}")
            return True

        stored_hash = processing_log[file_str].get('hash')
        if stored_hash != current_hash:
            print(f"  â†’ ë³€ê²½ëœ íŒŒì¼: {file_name}")
            return True
        else:
            print(f"  â†’ ë³€ê²½ ì—†ìŒ: {file_name}")
            return False

    except Exception as e:
        print(f"  â†’ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {file_name} - {str(e)}")
        return True  # ì˜¤ë¥˜ ì‹œ ì²˜ë¦¬ ì‹œë„

def scan_source_files():
    """ì†ŒìŠ¤ íŒŒì¼ ìŠ¤ìº”"""
    source_path = Path('data/source')
    print(f"ì†ŒìŠ¤ ë””ë ‰í„°ë¦¬ í™•ì¸: {source_path.absolute()}")

    if not source_path.exists():
        print(f"âŒ ì†ŒìŠ¤ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {source_path}")
        print("í˜„ì¬ ì‘ì—… ë””ë ‰í„°ë¦¬:", os.getcwd())
        print("ë””ë ‰í„°ë¦¬ ë‚´ìš©:", os.listdir('.') if os.path.exists('.') else "ì—†ìŒ")
        return []

    print(f"âœ… ì†ŒìŠ¤ ë””ë ‰í„°ë¦¬ ë°œê²¬: {source_path}")

    files = []
    total_files = 0

    for file_path in source_path.rglob('*'):
        if file_path.is_file():
            total_files += 1
            print(f"ë°œê²¬ëœ íŒŒì¼: {file_path}")

            # ë©”ëª¨ íŒŒì¼ì€ ë¯¸ë¦¬ ì œì™¸
            if not (file_path.name.startswith('_') and file_path.suffix.lower() in ['.md', '.mdx']):
                files.append(file_path)
                print(f"  â†’ ì²˜ë¦¬ ëŒ€ìƒì— ì¶”ê°€")
            else:
                print(f"  â†’ ë©”ëª¨ íŒŒì¼ë¡œ ì œì™¸")

    print(f"\nğŸ“Š ìŠ¤ìº” ê²°ê³¼:")
    print(f"  - ì „ì²´ íŒŒì¼: {total_files}ê°œ")
    print(f"  - ì²˜ë¦¬ ëŒ€ìƒ: {len(files)}ê°œ")

    if len(files) == 0:
        print("âš ï¸ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        print("data/source í´ë” êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

    return files

def parse_single_file(file_path):
    """ë‹¨ì¼ íŒŒì¼ íŒŒì‹±"""
    print(f"\nğŸ”„ íŒŒì‹± ì‹œì‘: {file_path}")

    try:
        file_ext = file_path.suffix.lower()

        if file_ext == '.pdf':
            result = parse_pdf(str(file_path))
        elif file_ext == '.docx':
            result = parse_docx(str(file_path))
        elif file_ext == '.pptx':
            result = parse_pptx(str(file_path))
        elif file_ext in ['.xlsx', '.xls']:
            result = parse_xlsx(str(file_path))
        elif file_ext in ['.md', '.mdx', '.js', '.ts']:
            # í…ìŠ¤íŠ¸ íŒŒì¼ì€ ê°„ë‹¨íˆ ì½ê¸°
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            result = {
                "content": content,
                "tables": [],
                "images": [],
                "metadata": {
                    "word_count": len(content.split()),
                    "char_count": len(content),
                    "file_type": file_ext[1:]
                }
            }
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}")

        # ê³µí†µ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        result.update({
            "file_path": str(file_path),
            "file_name": file_path.name,
            "parsing_method": "github_actions_python",
            "parsed_at": datetime.utcnow().isoformat(),
            "file_hash": get_file_hash(file_path)
        })

        print(f"âœ… íŒŒì‹± ì„±ê³µ: {file_path.name}")
        return result

    except Exception as e:
        print(f"âŒ íŒŒì‹± ì‹¤íŒ¨: {file_path} - {str(e)}")
        return {
            "file_path": str(file_path),
            "file_name": file_path.name,
            "parsing_method": "github_actions_python",
            "parsed_at": datetime.utcnow().isoformat(),
            "error": str(e),
            "status": "failed"
        }

def save_parsing_result(file_path, result):
    """íŒŒì‹± ê²°ê³¼ ì €ì¥"""
    # ìƒëŒ€ ê²½ë¡œë¥¼ processed ê²½ë¡œë¡œ ë³€í™˜
    relative_path = file_path.relative_to('data/source')
    output_path = Path('data/processed/parsed') / relative_path.with_suffix('.json')

    # ë””ë ‰í„°ë¦¬ ìƒì„±
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # ê²°ê³¼ ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    return output_path

def main():
    print("=" * 60)
    print("MCNC RAG Assistant - ë¬¸ì„œ ë°°ì¹˜ íŒŒì‹±")
    print("=" * 60)

    parser = argparse.ArgumentParser(description='ë¬¸ì„œ ë°°ì¹˜ íŒŒì‹±')
    parser.add_argument('--all', action='store_true', help='ëª¨ë“  íŒŒì¼ ì¬íŒŒì‹±')
    parser.add_argument('--new-only', action='store_true', help='ìƒˆë¡œìš´ íŒŒì¼ë§Œ íŒŒì‹±')
    parser.add_argument('file_path', nargs='?', help='íŠ¹ì • íŒŒì¼ íŒŒì‹±')

    args = parser.parse_args()
    print(f"ì‹¤í–‰ ì˜µì…˜: {args}")

    # ì²˜ë¦¬ ë¡œê·¸ ë¡œë“œ
    processing_log = load_processing_log()

    if args.file_path:
        # íŠ¹ì • íŒŒì¼ ì²˜ë¦¬
        file_path = Path(args.file_path)
        print(f"\níŠ¹ì • íŒŒì¼ ì²˜ë¦¬ ëª¨ë“œ: {file_path}")

        if not file_path.exists():
            print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            sys.exit(1)

        result = parse_single_file(file_path)
        output_path = save_parsing_result(file_path, result)

        # ë¡œê·¸ ì—…ë°ì´íŠ¸
        processing_log[str(file_path)] = {
            "hash": result.get("file_hash"),
            "parsed_at": result.get("parsed_at"),
            "output_path": str(output_path),
            "status": "success" if "error" not in result else "failed"
        }

    else:
        # ì „ì²´ íŒŒì¼ ìŠ¤ìº”
        print(f"\nì „ì²´ íŒŒì¼ ìŠ¤ìº” ëª¨ë“œ (ê°•ì œ ì¬ì²˜ë¦¬: {args.all})")
        source_files = scan_source_files()

        if not source_files:
            print("âŒ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            sys.exit(1)

        processed_count = 0
        failed_count = 0
        skipped_count = 0

        for file_path in source_files:
            # ì²˜ë¦¬ í•„ìš”ì„± í™•ì¸
            if not should_process_file(file_path, processing_log, force_all=args.all):
                skipped_count += 1
                continue

            result = parse_single_file(file_path)
            output_path = save_parsing_result(file_path, result)

            # ë¡œê·¸ ì—…ë°ì´íŠ¸
            processing_log[str(file_path)] = {
                "hash": result.get("file_hash"),
                "parsed_at": result.get("parsed_at"),
                "output_path": str(output_path),
                "status": "success" if "error" not in result else "failed"
            }

            if "error" in result:
                failed_count += 1
            else:
                processed_count += 1

        print(f"\nğŸ“Š ì²˜ë¦¬ ì™„ë£Œ ìš”ì•½:")
        print(f"  - ì„±ê³µ: {processed_count}ê°œ")
        print(f"  - ì‹¤íŒ¨: {failed_count}ê°œ")
        print(f"  - ìŠ¤í‚µ: {skipped_count}ê°œ")
        print(f"  - ì „ì²´: {len(source_files)}ê°œ")

    # ì²˜ë¦¬ ë¡œê·¸ ì €ì¥
    save_processing_log(processing_log)
    print("\nğŸ‰ ë°°ì¹˜ íŒŒì‹± ì™„ë£Œ!")

if __name__ == "__main__":
    main()